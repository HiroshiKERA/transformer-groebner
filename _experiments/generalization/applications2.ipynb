{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/kera/workspace/Transformer-GB\n"
     ]
    }
   ],
   "source": [
    "%cd \"/home/kera/workspace/Transformer-GB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-f\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import yaml \n",
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import argparse\n",
    "import re \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoConfig\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from src.loader.data import load_data\n",
    "from src.loader.checkpoint import load_trained_bag\n",
    "from src.evalution.evaluators import eval_prediction\n",
    "\n",
    "load('src/data/symbolic_utils.sage')\n",
    "load('src/data/gbdataset.sage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def sol_listform(sol):\n",
    "    return list(sol.values())[::-1]\n",
    "\n",
    "def print_solution(sols, ring):\n",
    "    for s in sols:\n",
    "        print([s[x] for x in ring.gens()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def coordinate_change_map(ring, num_bound=None):\n",
    "    while True:\n",
    "        n = ring.ngens()\n",
    "        if num_bound is not None:\n",
    "            P = matrix.random(ring.base_ring(), n, n, num_bound=num_bound)\n",
    "        else:\n",
    "            P = matrix.random(ring.base_ring(), n, n)\n",
    "            \n",
    "        if P.rank()  == n: break\n",
    "\n",
    "    gens = ring.gens()\n",
    "    trans_gens = matrix(gens) * P\n",
    "    trans_gens = trans_gens[0]\n",
    "    change_map = dict(zip(gens, trans_gens))\n",
    "    return change_map\n",
    "\n",
    "def coordinate_change(F, change_map):\n",
    "    return [f.subs(change_map) for f in F]\n",
    "\n",
    "def is_shape_position(G, verbose=False, relaxed_lt_condition=False):\n",
    "    ring = G.ring()\n",
    "    gens = ring.gens()\n",
    "\n",
    "    # if verbose:\n",
    "    #     print('Input polynomial set:')\n",
    "    #     print('---------------------------')\n",
    "    #     for i, g in enumerate(G): print(f' g_{i} = {g}')\n",
    "    #     print('---------------------------')\n",
    "\n",
    "    if not G.is_groebner(): \n",
    "        if verbose: print(f'[GB Error] Given G is not a GB.')\n",
    "        return False\n",
    "\n",
    "    xn = gens[-1]\n",
    "    s = len(G)\n",
    "\n",
    "    univariate_check = [G[-1].variables() == (xn,)]\n",
    "    if not (univariate_check): \n",
    "        if verbose: print(f'[Univariaticity Error] Last polynomial is not univariate in core variable. \\n {G[-1]}')\n",
    "        return False\n",
    "    \n",
    "    if relaxed_lt_condition:\n",
    "        lt_check = [g.lt().variables() == (x,) for g, x in zip(G[:-1], gens[:-1])]\n",
    "    else:\n",
    "        lt_check = [g.lt() == x for g, x in zip(G[:-1], gens[:-1])]\n",
    "    if not all(lt_check): \n",
    "        if verbose: \n",
    "            if relaxed_lt_condition:\n",
    "                print(f\"[Leading Term Error (relaxed)] First {s-1} polynomials are not all led by x_i^e\")\n",
    "            else:\n",
    "                print(f\"[Leading Term Error ] First {s-1} polynomials are not all led by x_i\")\n",
    "            for i, (g, success) in enumerate(zip(G[:-1], lt_check)):\n",
    "                if success:\n",
    "                    print(f' [ok] g_{i} = {g}')\n",
    "                else:\n",
    "                    print(f' [  ] g_{i} = {g}')\n",
    "\n",
    "        return False\n",
    "    \n",
    "    difference_form_check = [(g.lm() - g).variables() in ((xn,), ()) for g in G[:-1]]\n",
    "    if not all(difference_form_check): \n",
    "        if verbose: \n",
    "            print(f'[Difference form Error] First {s-1} polynomials are not all x_i - h({xn}) form.')\n",
    "            for i, (g, success) in enumerate(zip(G[:-1], difference_form_check)):\n",
    "                if success:\n",
    "                    print(f' [ok] g_{i} = {g}')\n",
    "                else:\n",
    "                    print(f' [  ] g_{i} = {g}')\n",
    "        return False\n",
    "\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "ring = PolynomialRing(QQ, 'x', 2, order='lex')\n",
    "x1, x2 = ring.gens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "p1 = ring(x1 - x2 + 1)\n",
    "p2 = ring((-x1-2*x2)^2 + (2*x1 - 3*x2)^2 + (4*x2)^2 - 8^2 - (-5)^2 - (-4)^2)\n",
    "F = [p1, p2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{x1: 2, x0: 1}, {x1: -25/13, x0: -38/13}]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I = ideal(F)\n",
    "G = I.groebner_basis()\n",
    "sols = I.variety()\n",
    "sols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_shape_position(G, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def shuffled_least_square_demo(n=3, num_points=6, base_field=QQ, num_symmetric_polys=4, num_bound=3):\n",
    "    ring = PolynomialRing(base_field, 'x', n, order='lex')\n",
    "    point_ring = PolynomialRing(base_field, 'z', num_points, order='lex')\n",
    "\n",
    "    xs = ring.gens()\n",
    "    zs = point_ring.gens()\n",
    "\n",
    "    # prepare symmetric polynomials\n",
    "    # symmetric_polys = [sum(zs),\n",
    "    #                    sum([z^2 for z in zs]),\n",
    "    #                    sum([z1*z2 for z1, z2 in zip(zs, zs[1:]+zs[:1])]),\n",
    "    #                    sum([z^3 for z in zs]),\n",
    "    #                    sum([z1*z2*z3 for z1, z2, z3 in zip(zs, zs[1:]+zs[:1], zs[2:]+zs[:2])]),\n",
    "    #                    ]\n",
    "    symmetric_polys = [sum([z^i for z in zs]) for i in range(1, 2*num_symmetric_polys+1)]\n",
    "    symmetric_polys =  symmetric_polys[:num_symmetric_polys]\n",
    "    symmetric_polys = ideal(symmetric_polys).basis\n",
    "    print('-- Symmetric polynomials ------------')\n",
    "    for i, sp in enumerate(symmetric_polys): print(f's_{i} = {sp}')\n",
    "    print('')\n",
    "\n",
    "    # setup the problem\n",
    "    kwargs = {'num_bound': num_bound} if base_field == QQ else {}\n",
    "    A = matrix.random(ring.base_ring(), num_points, n, **kwargs)\n",
    "    v = matrix.random(ring.base_ring(), n, 1, **kwargs)\n",
    "    y = A * v\n",
    "    P = random_permutation_matrix(num_points)\n",
    "    print('-- Problem setup (Av = Py) ----------')\n",
    "    print(f'A \\n{A}')\n",
    "    print(f\"y' = {y.T}\")\n",
    "    print(f\"v' = {v.T}\")\n",
    "    print(f'P \\n{P}')\n",
    "    print('')\n",
    "\n",
    "    F = []\n",
    "    for sp in symmetric_polys:\n",
    "        f1 = sp(*vector(A * matrix(xs).T))\n",
    "        f2 = sp(*vector((P * y).T))\n",
    "        F.append(f1 - f2)\n",
    "\n",
    "    I = ideal(F)\n",
    "    G = I.groebner_basis()\n",
    "    print('-- Original system ---------------')\n",
    "    for i, f in enumerate(F): print(f'f_{i} = {f}')\n",
    "    print('')\n",
    "\n",
    "    print('-- Gröbner basis -----------------')\n",
    "    for i, g in enumerate(G): print(f'g_{i} = {g}')\n",
    "\n",
    "    is_shape = is_shape_position(G, verbose=True)\n",
    "    print(f'[{is_shape}] <G> is in shape position')\n",
    "    print('')\n",
    "\n",
    "    dim = I.dimension()\n",
    "    print(f'The ideal has dimension {dim}.')\n",
    "    if dim != 0: \n",
    "        print('Thus, no solution exists.')\n",
    "\n",
    "    else:        \n",
    "        sols = I.variety()\n",
    "        print(f'{len(sols)} solutions found.')\n",
    "        print_solution(sols, ring)\n",
    "\n",
    "        print(f'true solution is ')\n",
    "        print(vector(v.T))\n",
    "\n",
    "    output = {'A': A, 'v': v, 'y': y, 'P': P, 'F': F, 'G': G, 'sols': sols, 'ring': ring, 'symmetric_polys': symmetric_polys, 'is_shape': is_shape, 'num_bound': num_bound}\n",
    "\n",
    "    return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Symmetric polynomials ------------\n",
      "s_0 = z0 + z1 + z2 + z3 + z4 + z5\n",
      "s_1 = z0^2 + z1^2 + z2^2 + z3^2 + z4^2 + z5^2\n",
      "s_2 = z0^3 + z1^3 + z2^3 + z3^3 + z4^3 + z5^3\n",
      "s_3 = z0^4 + z1^4 + z2^4 + z3^4 + z4^4 + z5^4\n",
      "s_4 = z0^5 + z1^5 + z2^5 + z3^5 + z4^5 + z5^5\n",
      "\n",
      "-- Problem setup (Av = Py) ----------\n",
      "A \n",
      "[ 1/2   -2    0 -1/2]\n",
      "[   0    0   -3    0]\n",
      "[   0    3   -3    0]\n",
      "[  -3   -1    0    3]\n",
      "[ 1/2   -3   -3  3/2]\n",
      "[  -2    0 -1/2  1/2]\n",
      "y' = [  5/2    -3  -9/2 -17/2     0 -13/2]\n",
      "v' = [   3 -1/2    1    0]\n",
      "P \n",
      "[0 0 1 0 0 0]\n",
      "[0 0 0 1 0 0]\n",
      "[0 0 0 0 0 1]\n",
      "[0 1 0 0 0 0]\n",
      "[0 0 0 0 1 0]\n",
      "[1 0 0 0 0 0]\n",
      "\n",
      "-- Original system ---------------\n",
      "f_0 = -4*x0 - 3*x1 - 19/2*x2 + 9/2*x3 + 20\n",
      "f_1 = 27/2*x0^2 + x0*x1 - x0*x2 - 19*x0*x3 + 23*x1^2 - 13*x1*x3 + 109/4*x2^2 - 19/2*x2*x3 + 47/4*x3^2 - 150\n",
      "f_2 = -139/4*x0^3 - 123/4*x0^2*x1 - 33/4*x0^2*x2 + 351/4*x0^2*x3 + 21/2*x0*x1^2 + 27*x0*x1*x2 + 87/2*x0*x1*x3 + 12*x0*x2^2 - 21/2*x0*x2*x3 - 315/4*x0*x3^2 - 9*x1^3 - 162*x1^2*x2 + 87/2*x1^2*x3 + 81*x1*x2*x3 - 195/4*x1*x3^2 - 649/8*x2^3 + 327/8*x2^2*x3 - 165/8*x2*x3^2 + 243/8*x3^3 + 3965/4\n",
      "f_3 = 777/8*x0^4 + 211/2*x0^3*x1 + 29/2*x0^3*x2 - 679/2*x0^3*x3 + 147/2*x0^2*x1^2 + 27*x0^2*x1*x2 - 669/2*x0^2*x1*x3 + 39/2*x0^2*x2^2 - 51/2*x0^2*x2*x3 + 1983/4*x0^2*x3^2 - 58*x0*x1^3 - 162*x0*x1^2*x2 - 39*x0*x1^2*x3 - 162*x0*x1*x2^2 + 162*x0*x1*x2*x3 + 561/2*x0*x1*x3^2 - 53*x0*x2^3 + 78*x0*x2^2*x3 - 75/2*x0*x2*x3^2 - 637/2*x0*x3^3 + 179*x1^4 - 158*x1^3*x3 + 972*x1^2*x2^2 - 486*x1^2*x2*x3 + 363/2*x1^2*x3^2 - 486*x1*x2^2*x3 + 243*x1*x2*x3^2 - 295/2*x1*x3^3 + 3889/16*x2^4 - 649/4*x2^3*x3 + 975/8*x2^2*x3^2 - 163/4*x2*x3^3 + 1379/16*x3^4 - 30141/4\n",
      "f_4 = -4399/16*x0^5 - 6505/16*x0^4*x1 - 655/16*x0^4*x2 + 20085/16*x0^4*x3 - 1015/4*x0^3*x1^2 + 45/2*x0^3*x1*x2 + 6445/4*x0^3*x1*x3 - 35/4*x0^3*x2^2 + 115/4*x0^3*x2*x3 - 19575/8*x0^3*x3^2 - 355/2*x0^2*x1^3 - 405/2*x0^2*x1^2*x2 + 3585/4*x0^2*x1^2*x3 - 405/2*x0^2*x1*x2^2 + 405/2*x0^2*x1*x2*x3 - 19875/8*x0^2*x1*x3^2 - 145/2*x0^2*x2^3 + 465/4*x0^2*x2^2*x3 - 525/8*x0^2*x2*x3^2 + 19545/8*x0^2*x3^3 + 455/2*x0*x1^4 + 810*x0*x1^3*x2 - 185*x0*x1^3*x3 + 1215*x0*x1^2*x2^2 - 1215*x0*x1^2*x2*x3 - 1965/4*x0*x1^2*x3^2 + 810*x0*x1*x2^3 - 1215*x0*x1*x2^2*x3 + 1215/2*x0*x1*x2*x3^2 + 6085/4*x0*x1*x3^3 + 1615/8*x0*x2^4 - 805/2*x0*x2^3*x3 + 300*x0*x2^2*x3^2 - 395/4*x0*x2*x3^3 - 19245/16*x0*x3^4 - 33*x1^5 - 2430*x1^4*x2 + 1165/2*x1^4*x3 + 2430*x1^3*x2*x3 - 1435/2*x1^3*x3^2 - 4860*x1^2*x2^3 + 3645*x1^2*x2^2*x3 - 3645/2*x1^2*x2*x3^2 + 2275/4*x1^2*x3^3 + 2430*x1*x2^3*x3 - 3645/2*x1*x2^2*x3^2 + 1215/2*x1*x2*x3^3 - 7705/16*x1*x3^4 - 23329/32*x2^5 + 19445/32*x2^4*x3 - 9725/16*x2^3*x3^2 + 4865/16*x2^2*x3^3 - 2435/32*x2*x3^4 + 8019/32*x3^5 + 927425/16\n",
      "\n",
      "-- Gröbner basis -----------------\n",
      "g_0 = x0 - 3\n",
      "g_1 = x1 + 1/2\n",
      "g_2 = x2 - 1\n",
      "g_3 = x3\n",
      "[True] <G> is in shape position\n",
      "\n",
      "The ideal has dimension 0.\n",
      "1 solutions found.\n",
      "[3, -1/2, 1, 0]\n",
      "true solution is \n",
      "(3, -1/2, 1, 0)\n"
     ]
    }
   ],
   "source": [
    "output = shuffled_least_square_demo(n=4, \n",
    "                                    num_points=6, \n",
    "                                    base_field=QQ, \n",
    "                                    num_symmetric_polys=5, \n",
    "                                    num_bound=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(n, field_name):    \n",
    "    \n",
    "    load_dir = f'results/shape_gb_lex/gb_dataset_n={n}_field={field_name}'\n",
    "    print(f'-- Loading model from {load_dir} ---------')\n",
    "\n",
    "    bag = load_trained_bag(load_dir, from_checkpoint=True)\n",
    "    model = bag['model'] \n",
    "    tokenizer = bag['tokenizer']\n",
    "    params = bag['params']\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def Transformer_GB_demo(model, tokenizer, F, G, ring, num_beams=1):\n",
    "    F_prefix = [poly_to_prefix(f) for f in F]\n",
    "    G_prefix = [poly_to_prefix(g) for g in G]\n",
    "    x_text = ' [SEP] '.join(F_prefix)\n",
    "    y_text = ' [SEP] '.join(G_prefix)\n",
    "\n",
    "    x = tokenizer(x_text, return_tensors='pt')['input_ids'].cuda()\n",
    "    # y = tokenizer(y_text, return_tensors='pt')['input_ids'].cuda()\n",
    "    output_ids = model.generate(x, max_length=1000, num_beams=num_beams, do_sample=False)\n",
    "    z_text = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    G_pred = [prefix_to_poly(zt, ring) for zt in z_text[0].split('[SEP]')]\n",
    "\n",
    "    print('-- Gröbner basis (answer) ------------')\n",
    "    for i, g in enumerate(G): print(f'g_{i} = {g}')\n",
    "    print('')\n",
    "\n",
    "    print('-- Gröbner basis (Transformer) -------')\n",
    "    for i, g in enumerate(G_pred): print(f'g_{i} = {g}')\n",
    "    print('')\n",
    "\n",
    "    sols = ideal(G_pred).variety()\n",
    "    print(f'{len(sols)} solutions found.')\n",
    "    print_solution(sols, ring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "レイノルズ作用素を使えば次数があがらないか"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Symmetric polynomials ------------\n",
      "s_0 = z0 + z1 + z2 + z3 + z4 + z5 + z6 + z7\n",
      "s_1 = z0^2 + z1^2 + z2^2 + z3^2 + z4^2 + z5^2 + z6^2 + z7^2\n",
      "s_2 = z0^3 + z1^3 + z2^3 + z3^3 + z4^3 + z5^3 + z6^3 + z7^3\n",
      "\n",
      "-- Problem setup (Av = Py) ----------\n",
      "A \n",
      "[   7  -99  -10]\n",
      "[  12  -71   29]\n",
      "[21/2   82  -53]\n",
      "[ -41   -5    8]\n",
      "[  63   17   73]\n",
      "[  92   45  -19]\n",
      "[ -41 47/2   99]\n",
      "[ -43   56  -15]\n",
      "y' = [   2703    4560   -6174     -21    5610   -1140 10635/2   -3657]\n",
      "v' = [ 18 -33  69]\n",
      "P \n",
      "[1 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 1 0]\n",
      "[0 0 0 0 0 0 0 1]\n",
      "[0 0 1 0 0 0 0 0]\n",
      "[0 0 0 1 0 0 0 0]\n",
      "[0 1 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 1 0 0]\n",
      "[0 0 0 0 1 0 0 0]\n",
      "\n",
      "-- Original system ---------------\n",
      "f_0 = 119/2*x0 + 97/2*x1 + 112*x2 - 14397/2\n",
      "f_1 = 71789/4*x0^2 + 2721*x0*x1 - 2339*x0*x2 + 110373/4*x1^2 - 7165*x1*x2 + 19530*x2^2 - 562558725/4\n",
      "f_2 = 6516917/8*x0^3 + 1730883*x0^2*x1 + 3346749/4*x0^2*x2 + 2948313/4*x0*x1^2 - 729087*x0*x1*x2 - 29487/2*x0*x2^2 - 3938681/8*x1^3 - 3812859/4*x1^2*x2 + 3061011/2*x1*x2^2 + 1224106*x2^3 - 1246010847867/8\n",
      "\n",
      "-- Gröbner basis -----------------\n",
      "g_0 = x0 + 268795539005930036373802208432643519354425768874247423617002199745036043948885433434006716407785684571337751624484981521456/33511065562203746393849285153712349270964915551244606228716545453731493056706238621979509425686620430030315947048741136076657129*x2^5 - 62147095744147283021869341430635897037331008740263088790429535487132793622547747627855988735679048643805414305095854582661624/33511065562203746393849285153712349270964915551244606228716545453731493056706238621979509425686620430030315947048741136076657129*x2^4 + 740584609243909718459236292629397024203508962407282422454668251635996629139213809170141041848821591503361917625080946725427012/3723451729133749599316587239301372141218323950138289580968505050414610339634026513553278825076291158892257327449860126230739681*x2^3 - 43424295799018819251882228816612820044973255892597328826924838451814288318256443834474510332171181221271726732289528730477005300/3723451729133749599316587239301372141218323950138289580968505050414610339634026513553278825076291158892257327449860126230739681*x2^2 + 408475779808263107605742200090033485035009426545790815287632555760293574454080686609368914877633164966034180832287865634570654380/1241150576377916533105529079767124047072774650046096526989501683471536779878008837851092941692097052964085775816620042076913227*x2 - 3785674862932579918666355872278570227406010082261615726836442496687904054508482239158553996517639138769108574027264431537125787562/1241150576377916533105529079767124047072774650046096526989501683471536779878008837851092941692097052964085775816620042076913227\n",
      "g_1 = x1 - 329759475687687364211159410345201843331718211299334468148693420305765868349663572975740198479654602721538066425914564959312/33511065562203746393849285153712349270964915551244606228716545453731493056706238621979509425686620430030315947048741136076657129*x2^5 + 76242313335603367830953109590161564406622577732900078000630048690400025165805999667163532572637183387761281467076357683883848/33511065562203746393849285153712349270964915551244606228716545453731493056706238621979509425686620430030315947048741136076657129*x2^4 - 908552252577579963882980606421631400826985221922336167753665174687459782139860240115946226598038859679382146364790027426039324/3723451729133749599316587239301372141218323950138289580968505050414610339634026513553278825076291158892257327449860126230739681*x2^3 + 53273105155497314339937992053370366859297087125969918870144904904803095978067183673221306489983201704446757537551071329141893100/3723451729133749599316587239301372141218323950138289580968505050414610339634026513553278825076291158892257327449860126230739681*x2^2 - 498253608949223262903790549452022153944554847807616303063738409880178465168483790998311918056693988567980539229477661042151333076/1241150576377916533105529079767124047072774650046096526989501683471536779878008837851092941692097052964085775816620042076913227*x2 + 4460066637534681917465732439584974970676375908787820822623389704834235747078409208467275675299566353010309045307346274300377824647/1241150576377916533105529079767124047072774650046096526989501683471536779878008837851092941692097052964085775816620042076913227\n",
      "g_2 = x2^6 - 1382839156933011595142027797967880468915999/4899595278744365131208409279962465757274*x2^5 + 358808998851945678831145908454921681122131799/9799190557488730262416818559924931514548*x2^4 - 6676621396693494636259407766925185225455538332/2449797639372182565604204639981232878637*x2^3 + 2267833531687883322857789966616782409096817590759/19598381114977460524833637119849863029096*x2^2 - 97354618001447677303415837098302096433905432216249/39196762229954921049667274239699726058192*x2 + 756161034370571484264541838345485226647794047914731/39196762229954921049667274239699726058192\n",
      "[True] <G> is in shape position\n",
      "\n",
      "The ideal has dimension 0.\n",
      "1 solutions found.\n",
      "[18, -33, 69]\n",
      "true solution is \n",
      "(18, -33, 69)\n"
     ]
    }
   ],
   "source": [
    "n = 3\n",
    "field = QQ\n",
    "field_name = 'QQ'\n",
    "num_bound = 100\n",
    "\n",
    "output = shuffled_least_square_demo(n=n,\n",
    "                                    num_points=8, \n",
    "                                    base_field=field, \n",
    "                                    num_symmetric_polys=n, \n",
    "                                    num_bound=num_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "282.23538440656597"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(1382839156933011595142027797967880468915999/4899595278744365131208409279962465757274)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5279484658/18705963"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QQ(282.23538440656597)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "282.23538440656597"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(QQ(282.23538440656597))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Loading model from results/shape_gb_lex/gb_dataset_n=3_field=QQ ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "WordLevel error: Missing [UNK] token from the vocabulary",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[502], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m load_model_and_tokenizer(n, field_name)\n\u001b[1;32m      2\u001b[0m F, G, ring, v \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m'\u001b[39m], output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mG\u001b[39m\u001b[38;5;124m'\u001b[39m], output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mring\u001b[39m\u001b[38;5;124m'\u001b[39m], output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m \u001b[43mTransformer_GB_demo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mInteger\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(True solution is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;241m.\u001b[39mT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[494], line 7\u001b[0m, in \u001b[0;36mTransformer_GB_demo\u001b[0;34m(model, tokenizer, F, G, ring, num_beams)\u001b[0m\n\u001b[1;32m      4\u001b[0m x_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m [SEP] \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(F_prefix)\n\u001b[1;32m      5\u001b[0m y_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m [SEP] \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(G_prefix)\n\u001b[0;32m----> 7\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# y = tokenizer(y_text, return_tensors='pt')['input_ids'].cuda()\u001b[39;00m\n\u001b[1;32m      9\u001b[0m output_ids \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(x, max_length\u001b[38;5;241m=\u001b[39mInteger(\u001b[38;5;241m1000\u001b[39m), num_beams\u001b[38;5;241m=\u001b[39mnum_beams, do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/mambaforge/envs/sage/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2790\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2788\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2789\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2790\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2791\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2792\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/mambaforge/envs/sage/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2896\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2877\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2878\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2893\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2894\u001b[0m     )\n\u001b[1;32m   2895\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2899\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2902\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2904\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2906\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2907\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2908\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2909\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2912\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2913\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2914\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2915\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/sage/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2969\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2959\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2960\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2961\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2962\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2966\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2967\u001b[0m )\n\u001b[0;32m-> 2969\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2972\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2984\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2985\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2986\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2987\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2988\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/sage/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:576\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_plus\u001b[39m(\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    556\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    574\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[1;32m    575\u001b[0m     batched_input \u001b[38;5;241m=\u001b[39m [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[0;32m--> 576\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[0;32m~/mambaforge/envs/sage/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:504\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;66;03m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_truncation_and_padding(\n\u001b[1;32m    497\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[1;32m    498\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    501\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    502\u001b[0m )\n\u001b[0;32m--> 504\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    516\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[1;32m    518\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    528\u001b[0m ]\n",
      "\u001b[0;31mException\u001b[0m: WordLevel error: Missing [UNK] token from the vocabulary"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model_and_tokenizer(n, field_name)\n",
    "F, G, ring, v = output['F'], output['G'], output['ring'], output['v']\n",
    "Transformer_GB_demo(model, tokenizer, F, G, ring, num_beams=1)\n",
    "print(f'(True solution is {v.T})')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 10.0",
   "language": "sage",
   "name": "sagemath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "sage",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
